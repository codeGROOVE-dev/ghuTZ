package ghutz

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"log/slog"
	"math"
	"net/http"
	"net/url"
	"os"
	"path/filepath"
	"regexp"
	"sort"
	"strconv"
	"strings"
	"sync"
	"time"

	"github.com/codeGROOVE-dev/retry"
	"google.golang.org/genai"
)

// SECURITY: GitHub token patterns for validation.
var (
	// GitHub Personal Access Token (classic) - ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.
	githubPATRegex = regexp.MustCompile(`^ghp_[a-zA-Z0-9]{36}$`)
	// GitHub App Installation Token - ghs_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.
	githubAppTokenRegex = regexp.MustCompile(`^ghs_[a-zA-Z0-9]{36}$`)
	// GitHub Fine-grained PAT - github_pat_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx.
	githubFineGrainedRegex = regexp.MustCompile(`^github_pat_[a-zA-Z0-9_]{82}$`)
	// GitHub username validation regex
	validUsernameRegex = regexp.MustCompile(`^[a-zA-Z0-9]([a-zA-Z0-9-]{0,37}[a-zA-Z0-9])?$`)
	// HTML tag removal regex
	htmlTagRegex = regexp.MustCompile(`<[^>]*>`)
	// Whitespace normalization regex
	whitespaceRegex = regexp.MustCompile(`\s+`)
	// Timezone extraction patterns
	timezoneDataAttrRegex = regexp.MustCompile(`data-timezone="([^"]+)"`)
	timezoneJSONRegex     = regexp.MustCompile(`"timezone":"([^"]+)"`)
	timezoneFieldRegex    = regexp.MustCompile(`timezone:([^,}]+)`)
)

type Detector struct {
	githubToken   string
	mapsAPIKey    string
	geminiAPIKey  string
	geminiModel   string
	gcpProject    string
	logger        *slog.Logger
	httpClient    *http.Client
	forceActivity bool
	cache         *OtterCache
}

// retryableHTTPDo performs an HTTP request with exponential backoff and jitter.
// The returned response body must be closed by the caller.
func (d *Detector) retryableHTTPDo(ctx context.Context, req *http.Request) (*http.Response, error) {
	var resp *http.Response
	var lastErr error

	err := retry.Do(
		func() error {
			var err error
			resp, err = d.httpClient.Do(req.WithContext(ctx)) //nolint:bodyclose // Body closed on error, returned open on success for caller
			if err != nil {
				// Network errors are retryable
				lastErr = err
				return err
			}

			// Check for rate limiting or server errors
			if resp.StatusCode == 429 || resp.StatusCode == 403 || resp.StatusCode >= 500 {
				body, readErr := io.ReadAll(resp.Body)
				closeErr := resp.Body.Close()
				if readErr != nil {
					d.logger.Debug("failed to read error response body", "error", readErr)
				}
				if closeErr != nil {
					d.logger.Debug("failed to close error response body", "error", closeErr)
				}
				lastErr = fmt.Errorf("HTTP %d: %s", resp.StatusCode, string(body))
				d.logger.Debug("retryable HTTP error",
					"status", resp.StatusCode,
					"url", req.URL.String(),
					"body", string(body))
				return lastErr
			}

			// Success - response body will be handled by caller
			return nil
		},
		retry.Context(ctx),
		retry.Attempts(5),
		retry.Delay(time.Second),
		retry.MaxDelay(2*time.Minute),
		retry.DelayType(retry.FullJitterBackoffDelay),
		retry.OnRetry(func(n uint, err error) {
			d.logger.Debug("retrying HTTP request",
				"attempt", n+1,
				"url", req.URL.String(),
				"error", err)
		}),
		retry.RetryIf(func(err error) bool {
			// Retry on network errors and rate limits
			return err != nil
		}),
	)

	if err != nil {
		return nil, fmt.Errorf("request failed after retries: %w", lastErr)
	}

	return resp, nil
}

// isValidGitHubToken validates GitHub token format for security.
func (d *Detector) isValidGitHubToken(token string) bool {
	// SECURITY: Validate token format to prevent injection attacks
	token = strings.TrimSpace(token)

	// Check against known GitHub token patterns
	return githubPATRegex.MatchString(token) ||
		githubAppTokenRegex.MatchString(token) ||
		githubFineGrainedRegex.MatchString(token)
}

func New(opts ...Option) *Detector {
	return NewWithLogger(slog.Default(), opts...)
}

func NewWithLogger(logger *slog.Logger, opts ...Option) *Detector {
	optHolder := &OptionHolder{}
	for _, opt := range opts {
		opt(optHolder)
	}

	// Initialize cache
	var cache *OtterCache
	var cacheDir string

	if optHolder.cacheDir != "" {
		// Use custom cache directory
		cacheDir = optHolder.cacheDir
	} else if userCacheDir, err := os.UserCacheDir(); err == nil {
		// Use default user cache directory
		cacheDir = filepath.Join(userCacheDir, "ghutz")
	} else {
		logger.Debug("could not determine user cache directory", "error", err)
	}

	if cacheDir != "" {
		var err error
		cache, err = NewOtterCache(cacheDir, 20*24*time.Hour, logger)
		if err != nil {
			logger.Warn("cache initialization failed", "error", err, "cache_dir", cacheDir)
			// Cache is optional, continue without it
			cache = nil
		}
	}

	return &Detector{
		githubToken:   optHolder.githubToken,
		mapsAPIKey:    optHolder.mapsAPIKey,
		geminiAPIKey:  optHolder.geminiAPIKey,
		geminiModel:   optHolder.geminiModel,
		gcpProject:    optHolder.gcpProject,
		logger:        logger,
		httpClient:    &http.Client{Timeout: 30 * time.Second},
		forceActivity: optHolder.forceActivity,
		cache:         cache,
	}
}

// Close properly shuts down the detector, including saving the cache to disk
func (d *Detector) Close() error {
	if d.cache != nil {
		return d.cache.Close()
	}
	return nil
}

func (d *Detector) Detect(ctx context.Context, username string) (*Result, error) {
	if username == "" {
		return nil, fmt.Errorf("username cannot be empty")
	}

	// Validate username to prevent injection attacks
	// GitHub usernames can only contain alphanumeric characters or hyphens
	// Cannot have multiple consecutive hyphens
	// Cannot begin or end with a hyphen
	// Maximum 39 characters
	if len(username) > 39 {
		return nil, fmt.Errorf("username too long (max 39 characters)")
	}

	if !validUsernameRegex.MatchString(username) {
		return nil, fmt.Errorf("invalid username format")
	}

	d.logger.Info("detecting timezone", "username", username)

	// Fetch user profile to get the full name
	var fullName string
	if user := d.fetchUser(ctx, username); user != nil && user.Name != "" {
		fullName = user.Name
		d.logger.Debug("fetched user full name", "username", username, "name", fullName)
	}

	// Always perform activity analysis for fun and comparison
	d.logger.Debug("performing activity pattern analysis", "username", username)
	activityResult := d.tryActivityPatterns(ctx, username)

	// Try quick detection methods first
	d.logger.Debug("trying profile HTML scraping", "username", username)
	if result := d.tryProfileScraping(ctx, username); result != nil {
		d.logger.Info("detected from profile HTML", "username", username, "timezone", result.Timezone)
		result.Name = fullName
		// Add activity data if we have it
		if activityResult != nil {
			result.ActivityTimezone = activityResult.ActivityTimezone
			result.QuietHoursUTC = activityResult.QuietHoursUTC
			result.ActiveHoursLocal = activityResult.ActiveHoursLocal
			result.LunchHoursLocal = activityResult.LunchHoursLocal
		}
		return result, nil
	}
	d.logger.Debug("profile HTML scraping failed", "username", username)

	d.logger.Debug("trying location field analysis", "username", username)
	if result := d.tryLocationField(ctx, username); result != nil {
		d.logger.Info("detected from location field", "username", username, "timezone", result.Timezone, "location", result.LocationName)
		result.Name = fullName
		// Add activity data if we have it
		if activityResult != nil {
			result.ActivityTimezone = activityResult.ActivityTimezone
			result.QuietHoursUTC = activityResult.QuietHoursUTC
			result.ActiveHoursLocal = activityResult.ActiveHoursLocal
			result.LunchHoursLocal = activityResult.LunchHoursLocal
		}
		return result, nil
	}
	d.logger.Debug("location field analysis failed", "username", username)

	d.logger.Debug("trying Gemini analysis with contextual data", "username", username, "has_activity_data", activityResult != nil)
	if result := d.tryUnifiedGeminiAnalysis(ctx, username, activityResult); result != nil {
		result.Name = fullName
		if activityResult != nil {
			// Preserve activity data in the final result
			result.ActivityTimezone = activityResult.ActivityTimezone
			result.QuietHoursUTC = activityResult.QuietHoursUTC
			result.ActiveHoursLocal = activityResult.ActiveHoursLocal
			result.LunchHoursLocal = activityResult.LunchHoursLocal
			d.logger.Info("timezone detected with Gemini + activity", "username", username,
				"activity_timezone", activityResult.Timezone, "final_timezone", result.Timezone)
		} else {
			d.logger.Info("timezone detected with Gemini only", "username", username, "timezone", result.Timezone)
		}
		return result, nil
	}
	d.logger.Debug("Gemini analysis failed", "username", username)

	if activityResult != nil {
		d.logger.Info("using activity-only result as fallback", "username", username, "timezone", activityResult.Timezone)
		activityResult.Name = fullName
		return activityResult, nil
	}

	return nil, fmt.Errorf("could not determine timezone for %s", username)
}

func (d *Detector) tryProfileScraping(ctx context.Context, username string) *Result {
	url := fmt.Sprintf("https://github.com/%s", username)

	req, err := http.NewRequestWithContext(ctx, "GET", url, http.NoBody)
	if err != nil {
		return nil
	}

	// SECURITY: Validate and sanitize GitHub token before use
	if d.githubToken != "" && d.isValidGitHubToken(d.githubToken) {
		req.Header.Set("Authorization", "token "+d.githubToken)
	}

	resp, err := d.cachedHTTPDo(ctx, req)
	if err != nil {
		return nil
	}
	defer func() {
		if err := resp.Body.Close(); err != nil {
			d.logger.Debug("failed to close response body", "error", err)
		}
	}()

	// Check if user exists - GitHub returns 404 for non-existent users
	if resp.StatusCode == http.StatusNotFound {
		d.logger.Info("GitHub user not found", "username", username)
		// Return a special result indicating user doesn't exist
		// This will be cached to avoid repeated lookups
		return &Result{
			Username:   username,
			Timezone:   "UTC", // Default timezone for non-existent users
			Confidence: 0,     // Zero confidence indicates non-existent user
			Method:     "user_not_found",
		}
	}

	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil
	}

	html := string(body)
	
	// Try extracting timezone from HTML using pre-compiled regex patterns
	patterns := []*regexp.Regexp{
		timezoneDataAttrRegex,
		timezoneFieldRegex,
		timezoneJSONRegex,
	}

	for _, re := range patterns {
		if matches := re.FindStringSubmatch(html); len(matches) > 1 {
			tz := strings.TrimSpace(matches[1])
			if tz != "" && tz != "UTC" {
				return &Result{
					Username:   username,
					Timezone:   tz,
					Confidence: 0.95,
					Method:     "github_profile",
				}
			}
		}
	}

	return nil
}

func (d *Detector) tryLocationField(ctx context.Context, username string) *Result {
	user := d.fetchUser(ctx, username)
	if user == nil || user.Location == "" {
		d.logger.Debug("no location field found", "username", username)
		return nil
	}

	d.logger.Debug("analyzing location field", "username", username, "location", user.Location)

	// Check if location is too vague for geocoding
	location := strings.ToLower(strings.TrimSpace(user.Location))
	vagueLocations := []string{
		"united states", "usa", "us", "america",
		"canada", "uk", "united kingdom", "britain",
		"germany", "france", "italy", "spain",
		"australia", "japan", "china", "india",
		"brazil", "russia", "mexico",
		"earth", "world", "planet earth",
	}

	for _, vague := range vagueLocations {
		if location == vague {
			d.logger.Debug("location too vague for geocoding", "username", username, "location", user.Location)
			return nil
		}
	}

	coords, err := d.geocodeLocation(ctx, user.Location)
	if err != nil {
		d.logger.Warn("geocoding failed - continuing without location data", "username", username, "location", user.Location, "error", err)
		return nil
	}

	d.logger.Debug("geocoded location", "username", username, "location", user.Location,
		"latitude", coords.Latitude, "longitude", coords.Longitude)

	timezone, err := d.timezoneForCoordinates(ctx, coords.Latitude, coords.Longitude)
	if err != nil {
		d.logger.Warn("timezone lookup failed - continuing without timezone data", "username", username, "coordinates",
			fmt.Sprintf("%.4f,%.4f", coords.Latitude, coords.Longitude), "error", err)
		return nil
	}

	d.logger.Debug("determined timezone from coordinates", "username", username,
		"location", user.Location, "timezone", timezone)

	return &Result{
		Username:     username,
		Timezone:     timezone,
		Location:     coords,
		LocationName: user.Location,
		Confidence:   0.8, // Higher confidence from API-based detection
		Method:       "location_geocoding",
	}
}

// tryUnifiedGeminiAnalysis uses Gemini with all available data (activity + context) in a single call.
func (d *Detector) tryUnifiedGeminiAnalysis(ctx context.Context, username string, activityResult *Result) *Result {
	// The SDK will automatically use API key if set, otherwise try Application Default Credentials
	// No need to explicitly check - let the SDK handle authentication

	// Gather contextual data about the user
	user := d.fetchUser(ctx, username)
	if user == nil {
		d.logger.Debug("could not fetch user data for Gemini analysis", "username", username)
		return nil
	}

	prs, err := d.fetchPullRequests(ctx, username)
	if err != nil {
		d.logger.Debug("failed to fetch pull requests", "username", username, "error", err)
		prs = []PullRequest{}
	}
	issues, err := d.fetchIssues(ctx, username)
	if err != nil {
		d.logger.Debug("failed to fetch issues", "username", username, "error", err)
		issues = []Issue{}
	}

	// Find longest PR/issue body for language analysis
	var longestBody string
	var longestTitle string
	for _, pr := range prs {
		if len(pr.Body) > len(longestBody) {
			longestBody = pr.Body
			longestTitle = pr.Title
		}
	}
	for _, issue := range issues {
		if len(issue.Body) > len(longestBody) {
			longestBody = issue.Body
			longestTitle = issue.Title
		}
	}

	// Limit body to 5000 chars for token efficiency
	if len(longestBody) > 5000 {
		longestBody = longestBody[:5000] + "..."
	}

	prSummary := make([]map[string]interface{}, len(prs))
	for i, pr := range prs {
		prSummary[i] = map[string]interface{}{
			"title": pr.Title,
		}
	}

	var websiteContent string
	if user.Blog != "" {
		d.logger.Debug("fetching website content for Gemini analysis", "username", username, "blog_url", user.Blog)
		websiteContent = d.fetchWebsiteContent(ctx, user.Blog)
	}

	orgs, err := d.fetchOrganizations(ctx, username)
	if err != nil {
		d.logger.Debug("failed to fetch organizations", "username", username, "error", err)
		orgs = []Organization{}
	}
	d.logger.Debug("fetched organization data", "username", username, "org_count", len(orgs))

	contextData := map[string]interface{}{
		"github_user_json":       user,
		"pull_requests":          prSummary,
		"website_content":        websiteContent,
		"organizations":          orgs,
		"longest_pr_issue_body":  longestBody,
		"longest_pr_issue_title": longestTitle,
		"issue_count":            len(issues),
	}

	var method string
	if activityResult != nil {
		contextData["activity_detected_timezone"] = activityResult.Timezone
		contextData["activity_confidence"] = activityResult.Confidence

		// Add work schedule information from activity analysis
		if activityResult.ActiveHoursLocal.Start != 0 || activityResult.ActiveHoursLocal.End != 0 {
			contextData["work_start_local"] = activityResult.ActiveHoursLocal.Start
			contextData["work_end_local"] = activityResult.ActiveHoursLocal.End
		}

		// Add lunch timing information
		if activityResult.LunchHoursLocal.Start != 0 || activityResult.LunchHoursLocal.End != 0 {
			contextData["lunch_start_local"] = activityResult.LunchHoursLocal.Start
			contextData["lunch_end_local"] = activityResult.LunchHoursLocal.End
			contextData["lunch_confidence"] = activityResult.LunchHoursLocal.Confidence
		}

		// Add quiet hours (sleep pattern) in UTC
		if len(activityResult.QuietHoursUTC) > 0 {
			contextData["sleep_hours_utc"] = activityResult.QuietHoursUTC
		}

		// Add GMT offset info instead of specific timezone candidates
		if strings.HasPrefix(activityResult.Timezone, "UTC") {
			// Extract offset from UTC format (e.g., "UTC+5", "UTC-8")
			offsetStr := strings.TrimPrefix(activityResult.Timezone, "UTC")
			if offset, err := strconv.Atoi(offsetStr); err == nil {
				contextData["detected_gmt_offset"] = fmt.Sprintf("GMT%+d", offset)
				contextData["detected_gmt_offset_note"] = fmt.Sprintf("Activity patterns suggest GMT%+d timezone. Consider major cities and tech hubs in this offset.", offset)
			}
		}

		method = "gemini_refined_activity"
		d.logger.Debug("analyzing with Gemini + activity data", "username", username,
			"activity_timezone", activityResult.Timezone, "profile_location", user.Location,
			"company", user.Company, "website_available", websiteContent != "")
	} else {
		method = "gemini_analysis"
		d.logger.Debug("analyzing with Gemini only", "username", username,
			"profile_location", user.Location, "company", user.Company, "website_available", websiteContent != "")
	}

	timezone, location, confidence, err := d.queryUnifiedGeminiForTimezone(ctx, contextData, true)
	if err != nil {
		d.logger.Debug("Gemini analysis failed", "username", username, "error", err)
		return nil
	}

	if timezone == "" {
		d.logger.Debug("Gemini could not determine timezone", "username", username)
		return nil
	}

	result := &Result{
		Username:                username,
		Timezone:                timezone,
		GeminiSuggestedLocation: location,
		Confidence:              confidence,
		Method:                  method,
	}

	// Try to geocode the AI-suggested location to get coordinates for the map
	if location != "" {
		d.logger.Debug("geocoding AI-suggested location", "username", username, "location", location)
		if coords, err := d.geocodeLocation(ctx, location); err == nil {
			result.Location = coords
			result.LocationName = location
			d.logger.Debug("successfully geocoded AI location", "username", username, "location", location, "lat", coords.Latitude, "lng", coords.Longitude)
		} else {
			d.logger.Debug("failed to geocode AI location", "username", username, "location", location, "error", err)
		}
	}

	return result
}


func (d *Detector) geocodeLocation(ctx context.Context, location string) (*Location, error) {
	if d.mapsAPIKey == "" {
		d.logger.Warn("Google Maps API key not configured - skipping geocoding", "location", location)
		return nil, fmt.Errorf("Google Maps API key not configured")
	}

	encodedLocation := url.QueryEscape(location)
	url := fmt.Sprintf("https://maps.googleapis.com/maps/api/geocode/json?address=%s&key=%s",
		encodedLocation, d.mapsAPIKey)

	req, err := http.NewRequestWithContext(ctx, "GET", url, http.NoBody)
	if err != nil {
		return nil, err
	}

	resp, err := d.cachedHTTPDo(ctx, req)
	if err != nil {
		return nil, err
	}
	defer func() {
		if err := resp.Body.Close(); err != nil {
			d.logger.Debug("failed to close response body", "error", err)
		}
	}()

	var result struct {
		Results []struct {
			Geometry struct {
				Location struct {
					Lat float64 `json:"lat"`
					Lng float64 `json:"lng"`
				} `json:"location"`
				LocationType string `json:"location_type"`
			} `json:"geometry"`
			Types            []string `json:"types"`
			FormattedAddress string   `json:"formatted_address"`
		} `json:"results"`
		Status string `json:"status"`
	}

	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil, err
	}

	d.logger.Debug("geocoding API raw response", "location", location, "status", resp.StatusCode,
		"content_type", resp.Header.Get("Content-Type"), "body_preview", string(body[:min(200, len(body))]))

	if err := json.Unmarshal(body, &result); err != nil {
		d.logger.Debug("geocoding JSON parse error", "location", location, "error", err, "full_body", string(body))
		return nil, fmt.Errorf("failed to parse geocoding response: %w", err)
	}

	if result.Status != "OK" || len(result.Results) == 0 {
		return nil, fmt.Errorf("geocoding failed: %s", result.Status)
	}

	firstResult := result.Results[0]

	locationType := firstResult.Geometry.LocationType
	d.logger.Debug("geocoding result precision", "location", location,
		"location_type", locationType, "types", firstResult.Types,
		"formatted_address", firstResult.FormattedAddress)

	// APPROXIMATE results are often geographic centers of large areas and unreliable for timezone detection
	if locationType == "APPROXIMATE" {
		hasCountryType := false
		hasPreciseType := false
		for _, t := range firstResult.Types {
			if t == "country" || t == "administrative_area_level_1" {
				hasCountryType = true
			}
			if t == "locality" || t == "sublocality" || t == "neighborhood" || t == "street_address" {
				hasPreciseType = true
			}
		}

		if hasCountryType && !hasPreciseType {
			d.logger.Debug("rejecting imprecise geocoding result", "location", location,
				"location_type", locationType, "reason", "country-level approximate result")
			return nil, fmt.Errorf("location too imprecise for reliable timezone detection: %s", location)
		}
	}

	coords := &Location{
		Latitude:  firstResult.Geometry.Location.Lat,
		Longitude: firstResult.Geometry.Location.Lng,
	}

	return coords, nil
}

func (d *Detector) timezoneForCoordinates(ctx context.Context, lat, lng float64) (string, error) {
	if d.mapsAPIKey == "" {
		d.logger.Warn("Google Maps API key not configured - skipping timezone lookup", "lat", lat, "lng", lng)
		return "", fmt.Errorf("Google Maps API key not configured")
	}

	timestamp := time.Now().Unix()
	url := fmt.Sprintf("https://maps.googleapis.com/maps/api/timezone/json?location=%.6f,%.6f&timestamp=%d&key=%s",
		lat, lng, timestamp, d.mapsAPIKey)

	req, err := http.NewRequestWithContext(ctx, "GET", url, http.NoBody)
	if err != nil {
		return "", err
	}

	resp, err := d.cachedHTTPDo(ctx, req)
	if err != nil {
		return "", err
	}
	defer func() {
		if err := resp.Body.Close(); err != nil {
			d.logger.Debug("failed to close response body", "error", err)
		}
	}()

	var result struct {
		TimeZoneID string `json:"timeZoneId"`
		Status     string `json:"status"`
	}

	if err := json.NewDecoder(resp.Body).Decode(&result); err != nil {
		return "", err
	}

	if result.Status != "OK" {
		return "", fmt.Errorf("timezone lookup failed: %s", result.Status)
	}

	return result.TimeZoneID, nil
}

// calculateTypicalActiveHours determines typical work hours based on activity patterns
// It uses percentiles to exclude outliers (e.g., occasional early starts or late nights)
func calculateTypicalActiveHours(hourCounts map[int]int, quietHours []int, utcOffset int) (start, end int) {
	// Create a map for easy lookup of quiet hours
	quietMap := make(map[int]bool)
	for _, h := range quietHours {
		quietMap[h] = true
	}

	// Find hours with meaningful activity (>10% of max activity)
	maxActivity := 0
	for _, count := range hourCounts {
		if count > maxActivity {
			maxActivity = count
		}
	}
	threshold := maxActivity / 10

	// Collect active hours (not in quiet period and above threshold)
	var activeHours []int
	for hour := 0; hour < 24; hour++ {
		if !quietMap[hour] && hourCounts[hour] > threshold {
			activeHours = append(activeHours, hour)
		}
	}

	if len(activeHours) == 0 {
		// Default to 9am-5pm if no clear pattern
		return 9, 17
	}

	// Find the continuous block of active hours
	// Handle wrap-around (e.g., activity from 22-02)
	sort.Ints(activeHours)

	// Find the largest gap to determine where the active period starts/ends
	maxGap := 0
	gapStart := activeHours[len(activeHours)-1]
	for i := 0; i < len(activeHours); i++ {
		gap := activeHours[i] - gapStart
		if gap < 0 {
			gap += 24
		}
		if gap > maxGap {
			maxGap = gap
			start = activeHours[i]
		}
		gapStart = activeHours[i]
	}

	// Find the end of the active period
	end = start
	for i := 0; i < len(activeHours); i++ {
		hour := activeHours[i]
		// Check if this hour is part of the continuous block
		diff := hour - start
		if diff < 0 {
			diff += 24
		}
		if diff < 16 { // Maximum 16-hour workday
			end = hour
		}
	}

	// Apply smart filtering: use 10th and 90th percentiles to exclude outliers
	// This prevents occasional early/late activity from skewing the results
	activityInRange := make([]int, 0)
	for h := start; ; h = (h + 1) % 24 {
		if hourCounts[h] > 0 {
			// Add this hour's count multiple times to weight the calculation
			for i := 0; i < hourCounts[h]; i++ {
				activityInRange = append(activityInRange, h)
			}
		}
		if h == end {
			break
		}
	}

	if len(activityInRange) > 10 {
		sort.Ints(activityInRange)
		// Use 10th percentile for start (ignore occasional early starts)
		percentile10 := len(activityInRange) / 10
		// Use 90th percentile for end (ignore occasional late nights)
		percentile90 := len(activityInRange) * 9 / 10

		start = activityInRange[percentile10]
		end = activityInRange[percentile90]
	}

	// Convert from UTC to local time
	start = (start + utcOffset + 24) % 24
	end = (end + utcOffset + 24) % 24

	return start, end
}

// findSleepHours looks for extended periods of zero or near-zero activity
// This is more reliable than finding "quiet" hours which might just be evening time
func findSleepHours(hourCounts map[int]int) []int {
	// First, find all hours with zero or minimal activity
	zeroHours := []int{}
	for hour := 0; hour < 24; hour++ {
		if hourCounts[hour] <= 1 { // Allow for 1 random event
			zeroHours = append(zeroHours, hour)
		}
	}

	// If we have a good stretch of zero activity, use that
	if len(zeroHours) >= 5 {
		// Find the longest consecutive sequence
		maxLen := 0
		maxStart := 0
		currentStart := zeroHours[0]
		currentLen := 1

		for i := 1; i < len(zeroHours); i++ {
			if zeroHours[i] == zeroHours[i-1]+1 || (zeroHours[i-1] == 23 && zeroHours[i] == 0) {
				currentLen++
			} else {
				if currentLen > maxLen {
					maxLen = currentLen
					maxStart = currentStart
				}
				currentStart = zeroHours[i]
				currentLen = 1
			}
		}
		if currentLen > maxLen {
			maxLen = currentLen
			maxStart = currentStart
		}

		// Extract the core sleep hours from the zero-activity period
		// Skip early evening hours and wake-up hours to focus on deep sleep
		result := []int{}
		sleepStart := maxStart
		sleepLength := maxLen

		// If we have a long zero period (8+ hours), it likely includes evening time
		// Skip the first 2-3 hours to avoid evening time, and the last hour for wake-up
		if maxLen >= 8 {
			sleepStart = (maxStart + 3) % 24 // Skip first 3 hours (evening)
			sleepLength = maxLen - 4         // Also skip last hour (wake-up)
		} else if maxLen >= 6 {
			sleepStart = (maxStart + 1) % 24 // Skip first hour
			sleepLength = maxLen - 2         // Also skip last hour
		}

		// Limit to reasonable sleep duration (4-7 hours)
		if sleepLength > 7 {
			sleepLength = 7
		}
		if sleepLength < 4 {
			sleepLength = maxLen // Use original if adjustment made it too short
			sleepStart = maxStart
		}

		for i := 0; i < sleepLength; i++ {
			hour := (sleepStart + i) % 24
			result = append(result, hour)
		}

		if len(result) >= 4 {
			return result
		}
	}

	// Fall back to the old method if we don't have clear zero periods
	return findQuietHours(hourCounts)
}

func findQuietHours(hourCounts map[int]int) []int {
	minSum := 999999
	minStart := 0
	windowSize := 6

	for start := 0; start < 24; start++ {
		sum := 0
		for i := 0; i < windowSize; i++ {
			hour := (start + i) % 24
			sum += hourCounts[hour]
		}
		if sum < minSum {
			minSum = sum
			minStart = start
		}
	}

	quietHours := make([]int, windowSize)
	for i := 0; i < windowSize; i++ {
		quietHours[i] = (minStart + i) % 24
	}

	return quietHours
}

func timezoneFromOffset(offsetHours int) string {
	// Return generic UTC offset format since we don't know the country at this stage
	// This is used for activity-only detection where location is unknown
	if offsetHours >= 0 {
		return fmt.Sprintf("UTC+%d", offsetHours)
	}
	return fmt.Sprintf("UTC%d", offsetHours) // Negative sign is already included
}

func (d *Detector) queryUnifiedGeminiForTimezone(ctx context.Context, contextData map[string]interface{}, verbose bool) (string, string, float64, error) {
	// Check if we have activity data for confidence scoring later
	activityTimezone := ""
	hasActivityData := false
	if tz, ok := contextData["activity_detected_timezone"].(string); ok && tz != "" {
		activityTimezone = tz
		hasActivityData = true
	}

	// Use the single consolidated prompt for all cases
	prompt := unifiedGeminiPrompt()

	// Format evidence in a clear, readable way instead of a massive JSON blob
	formattedEvidence := d.formatEvidenceForGemini(contextData)

	// The consolidated prompt only takes one parameter - the formatted evidence
	fullPrompt := fmt.Sprintf(prompt, formattedEvidence)

	if hasActivityData {
		d.logger.Debug("Gemini unified prompt (with activity)", "prompt_length", len(fullPrompt),
			"evidence_length", len(formattedEvidence), "activity_timezone", activityTimezone)
	} else {
		d.logger.Debug("Gemini unified prompt (context only)", "prompt_length", len(fullPrompt),
			"evidence_length", len(formattedEvidence))
	}

	if verbose {
		d.logger.Debug("Gemini full prompt", "full_prompt", fullPrompt)
	}

	// Use the official genai SDK (handles both API key and ADC)
	geminiResp, err := d.callGeminiWithSDK(ctx, fullPrompt, verbose)
	if err != nil {
		return "", "", 0, fmt.Errorf("Gemini API call failed: %w", err)
	}
	return geminiResp.timezone, geminiResp.location, geminiResp.confidence, nil
}

type geminiResponse struct {
	timezone   string
	location   string
	confidence float64
	reasoning  string
}

// callGeminiWithSDK uses the official Google AI SDK which handles authentication automatically
func (d *Detector) callGeminiWithSDK(ctx context.Context, prompt string, verbose bool) (*geminiResponse, error) {
	// Check cache first if available
	cacheKey := fmt.Sprintf("genai:%s:%s", d.geminiModel, prompt)
	if d.cache != nil {
		if cachedData, found := d.cache.APICall(cacheKey, []byte(prompt)); found {
			d.logger.Debug("Gemini SDK cache hit")
			var result geminiResponse
			if err := json.Unmarshal(cachedData, &result); err == nil {
				return &result, nil
			}
		}
	}

	// Create client based on authentication method
	var client *genai.Client
	var err error
	var config *genai.ClientConfig

	if d.geminiAPIKey != "" {
		// When using API key, use Gemini API backend (not Vertex AI)
		// API keys work with Gemini API, not Vertex AI
		config = &genai.ClientConfig{
			Backend: genai.BackendGeminiAPI,
			APIKey:  d.geminiAPIKey,
		}
		d.logger.Info("Using Gemini API with API key")
	} else {
		// When using ADC, use Vertex AI backend
		projectID := d.gcpProject
		if projectID == "" {
			// Try to get from environment
			projectID = os.Getenv("GCP_PROJECT")
			if projectID == "" {
				projectID = os.Getenv("GOOGLE_CLOUD_PROJECT")
			}
			if projectID == "" {
				// Default project for ghuTZ
				projectID = "ghutz-468911"
			}
		}

		config = &genai.ClientConfig{
			Backend:  genai.BackendVertexAI,
			Project:  projectID,
			Location: "us-central1",
		}
		d.logger.Info("Using Vertex AI with Application Default Credentials", "project", projectID, "location", "us-central1")
	}

	client, err = genai.NewClient(ctx, config)

	if err != nil {
		return nil, fmt.Errorf("failed to create genai client: %w", err)
	}

	// Select model for Vertex AI
	modelName := d.geminiModel
	if modelName == "" {
		modelName = "gemini-2.5-flash-lite"
	}

	// Vertex AI expects the model name without "models/" prefix
	modelName = strings.TrimPrefix(modelName, "models/")

	d.logger.Debug("Using model", "model", modelName)

	// Prepare content with user role
	contents := []*genai.Content{
		{
			Role: "user",
			Parts: []*genai.Part{
				{Text: prompt},
			},
		},
	}

	// Configure generation
	maxTokens := int32(100)
	if verbose {
		maxTokens = 300
	}

	temperature := float32(0.1)
	genConfig := &genai.GenerateContentConfig{
		Temperature:      &temperature,
		MaxOutputTokens:  maxTokens,
		ResponseMIMEType: "application/json",
		ResponseSchema: &genai.Schema{
			Type: genai.TypeObject,
			Properties: map[string]*genai.Schema{
				"timezone": {
					Type:        genai.TypeString,
					Description: "IANA timezone identifier",
				},
				"location": {
					Type:        genai.TypeString,
					Description: "Specific location name (city, region) - NEVER return UNKNOWN",
				},
				"confidence": {
					Type:        genai.TypeString,
					Description: "Confidence level: high, medium, or low",
				},
				"reasoning": {
					Type:        genai.TypeString,
					Description: "Brief explanation of the decision and evidence used",
				},
			},
			Required: []string{"timezone", "location", "confidence", "reasoning"},
		},
	}

	// Generate content
	resp, err := client.Models.GenerateContent(ctx, modelName, contents, genConfig)
	if err != nil {
		return nil, fmt.Errorf("failed to generate content: %w", err)
	}

	// Parse response
	if len(resp.Candidates) == 0 || len(resp.Candidates[0].Content.Parts) == 0 {
		return nil, fmt.Errorf("no response from Gemini")
	}

	// The response should be JSON text
	jsonText := ""
	if textPart := resp.Candidates[0].Content.Parts[0]; textPart != nil && textPart.Text != "" {
		jsonText = textPart.Text
	} else {
		return nil, fmt.Errorf("unexpected response type from Gemini")
	}

	var result struct {
		Timezone   string `json:"timezone"`
		Location   string `json:"location"`
		Confidence string `json:"confidence"`
		Reasoning  string `json:"reasoning"`
	}

	if err := json.Unmarshal([]byte(jsonText), &result); err != nil {
		return nil, fmt.Errorf("failed to parse Gemini response: %w", err)
	}

	// Convert confidence to float
	var confidence float64
	switch strings.ToLower(result.Confidence) {
	case "high":
		confidence = 0.9
	case "medium":
		confidence = 0.7
	case "low":
		confidence = 0.5
	default:
		confidence = 0.6
	}

	d.logger.Info("Gemini detection via SDK successful",
		"timezone", result.Timezone,
		"location", result.Location,
		"confidence", confidence,
		"reasoning", result.Reasoning)

	response := &geminiResponse{
		timezone:   result.Timezone,
		location:   result.Location,
		confidence: confidence,
		reasoning:  result.Reasoning,
	}

	// Cache the successful response
	if d.cache != nil {
		if responseData, err := json.Marshal(response); err == nil {
			if err := d.cache.SetAPICall(cacheKey, []byte(prompt), responseData); err != nil {
				d.logger.Error("Failed to cache Gemini SDK response", "error", err)
			} else {
				d.logger.Info("Gemini SDK response cached for 20 days")
			}
		}
	}

	return response, nil
}

func (d *Detector) fetchWebsiteContent(ctx context.Context, blogURL string) string {
	if blogURL == "" {
		return ""
	}

	if !strings.HasPrefix(blogURL, "http://") && !strings.HasPrefix(blogURL, "https://") {
		blogURL = "https://" + blogURL
	}

	req, err := http.NewRequestWithContext(ctx, "GET", blogURL, http.NoBody)
	if err != nil {
		d.logger.Debug("failed to create website request", "url", blogURL, "error", err)
		return ""
	}

	req.Header.Set("User-Agent", "GitHub-Timezone-Detector/1.0")

	resp, err := d.cachedHTTPDo(ctx, req)
	if err != nil {
		d.logger.Debug("failed to fetch website", "url", blogURL, "error", err)
		return ""
	}
	defer func() {
		if err := resp.Body.Close(); err != nil {
			d.logger.Debug("failed to close response body", "error", err)
		}
	}()

	if resp.StatusCode != 200 {
		d.logger.Debug("website returned non-200 status", "url", blogURL, "status", resp.StatusCode)
		return ""
	}

	body, err := io.ReadAll(io.LimitReader(resp.Body, 1024*1024)) // 1MB limit
	if err != nil {
		d.logger.Debug("failed to read website body", "url", blogURL, "error", err)
		return ""
	}

	return string(body)
}

func detectLunchBreak(hourCounts map[int]int, utcOffset int, workStart, workEnd int) (lunchStart, lunchEnd, confidence float64) {
	// Convert hour counts to 30-minute buckets for better precision
	bucketCounts := make(map[float64]int)
	for hour, count := range hourCounts {
		// Distribute the count evenly between two 30-minute buckets
		bucketCounts[float64(hour)] += count / 2
		bucketCounts[float64(hour)+0.5] += count / 2
		// Handle odd counts
		if count%2 == 1 {
			bucketCounts[float64(hour)] += 1
		}
	}

	// Look for activity dips during typical lunch hours (10am-3pm local for broader search)
	typicalLunchStart := 10.0
	typicalLunchEnd := 15.0

	// Convert local lunch hours to UTC
	lunchStartUTC := typicalLunchStart - float64(utcOffset)
	lunchEndUTC := typicalLunchEnd - float64(utcOffset)

	// Normalize to 0-24 range
	for lunchStartUTC < 0 {
		lunchStartUTC += 24
	}
	for lunchEndUTC < 0 {
		lunchEndUTC += 24
	}
	for lunchStartUTC >= 24 {
		lunchStartUTC -= 24
	}
	for lunchEndUTC >= 24 {
		lunchEndUTC -= 24
	}

	// Calculate average activity during work hours for comparison
	totalActivity := 0
	bucketCount := 0
	workHourBuckets := make([]float64, 0)
	for bucket := float64(workStart); bucket < float64(workEnd); bucket += 0.5 {
		utcBucket := bucket - float64(utcOffset)
		for utcBucket < 0 {
			utcBucket += 24
		}
		for utcBucket >= 24 {
			utcBucket -= 24
		}
		totalActivity += bucketCounts[utcBucket]
		bucketCount++
		workHourBuckets = append(workHourBuckets, utcBucket)
	}

	avgActivity := 0.0
	if bucketCount > 0 {
		avgActivity = float64(totalActivity) / float64(bucketCount)
	}

	// Find all candidate lunch periods (30-minute and 1-hour windows)
	type lunchCandidate struct {
		start      float64
		end        float64
		avgDip     float64
		distFrom12 float64
		confidence float64
	}

	candidates := make([]lunchCandidate, 0)

	// Check all possible 30-minute and 1-hour windows in the lunch timeframe
	for windowStart := lunchStartUTC; ; windowStart += 0.5 {
		if windowStart >= 24 {
			windowStart -= 24
		}

		// Try both 30-minute (1-bucket) and 60-minute (2-bucket) windows
		for windowSize := 1; windowSize <= 2; windowSize++ {
			windowEnd := windowStart + float64(windowSize)*0.5
			if windowEnd >= 24 {
				windowEnd -= 24
			}

			// Calculate average activity in this window
			windowActivity := 0.0
			windowBuckets := 0
			for bucket := windowStart; windowBuckets < windowSize; bucket += 0.5 {
				if bucket >= 24 {
					bucket -= 24
				}
				windowActivity += float64(bucketCounts[bucket])
				windowBuckets++
				if windowBuckets >= windowSize {
					break
				}
			}

			if windowBuckets > 0 {
				avgWindowActivity := windowActivity / float64(windowBuckets)

				// Calculate the dip relative to average work activity
				var dipPercentage float64
				if avgActivity > 0 {
					dipPercentage = (avgActivity - avgWindowActivity) / avgActivity
				}

				// Convert window center to local time to check distance from 12pm
				windowCenter := windowStart + float64(windowSize)*0.25
				localCenter := windowCenter + float64(utcOffset)
				for localCenter < 0 {
					localCenter += 24
				}
				for localCenter >= 24 {
					localCenter -= 24
				}
				distanceFrom12 := math.Abs(localCenter - 12.0)
				if distanceFrom12 > 12 {
					distanceFrom12 = 24 - distanceFrom12
				}

				// Calculate confidence based on dip size and proximity to 12pm
				confidence := 0.1 // Base confidence - always show something

				// Dip size component (max 0.6)
				dipComponent := 0.0
				if dipPercentage > 0.1 {
					dipComponent += 0.2 // Small dip
				}
				if dipPercentage > 0.25 {
					dipComponent += 0.2 // Significant dip
				}
				if dipPercentage > 0.5 {
					dipComponent += 0.2 // Very large dip
				}

				// Proximity to 12pm component (max 0.2)
				proximityComponent := 0.0
				if distanceFrom12 <= 2.0 {
					proximityComponent = (2.0 - distanceFrom12) / 2.0 * 0.2
				}

				// Duration appropriateness component (max 0.1)
				durationComponent := 0.0
				if windowSize == 1 && dipPercentage > 0.3 {
					// 30-minute breaks with strong dip pattern
					durationComponent = 0.1
				} else if windowSize == 2 && dipPercentage > 0.2 {
					// 1-hour breaks with reasonable dip pattern
					durationComponent = 0.1
				}

				// Combine components and cap at 1.0
				confidence = confidence + dipComponent + proximityComponent + durationComponent
				if confidence > 1.0 {
					confidence = 1.0
				}

				candidates = append(candidates, lunchCandidate{
					start:      windowStart + float64(utcOffset),
					end:        windowEnd + float64(utcOffset),
					avgDip:     dipPercentage,
					distFrom12: distanceFrom12,
					confidence: confidence,
				})
			}
		}

		// Stop when we've covered the lunch window
		if (lunchEndUTC > lunchStartUTC && windowStart >= lunchEndUTC) ||
			(lunchEndUTC < lunchStartUTC && windowStart >= lunchEndUTC && windowStart < lunchStartUTC) {
			break
		}
	}

	// Find the best candidate (highest confidence, prefer closer to 12pm for ties)
	bestCandidate := lunchCandidate{start: 12.0, end: 13.0, confidence: 0.1} // Default fallback

	for _, candidate := range candidates {
		// Normalize candidate times to 0-24 range
		for candidate.start < 0 {
			candidate.start += 24
		}
		for candidate.start >= 24 {
			candidate.start -= 24
		}
		for candidate.end < 0 {
			candidate.end += 24
		}
		for candidate.end >= 24 {
			candidate.end -= 24
		}

		// Prefer higher confidence, with proximity to 12pm as tiebreaker
		if candidate.confidence > bestCandidate.confidence ||
			(candidate.confidence == bestCandidate.confidence && candidate.distFrom12 < bestCandidate.distFrom12) {
			bestCandidate = candidate
		}
	}

	return bestCandidate.start, bestCandidate.end, bestCandidate.confidence
}

// formatEvidenceForGemini formats contextual data into a readable, structured format for Gemini analysis
func (d *Detector) formatEvidenceForGemini(contextData map[string]interface{}) string {
	var evidence strings.Builder

	// ACTIVITY ANALYSIS SECTION
	if activityTz, ok := contextData["activity_detected_timezone"].(string); ok {
		evidence.WriteString("## ACTIVITY ANALYSIS (HIGHLY RELIABLE)\n")
		evidence.WriteString(fmt.Sprintf("Detected Timezone: %s\n", activityTz))

		if confidence, ok := contextData["activity_confidence"].(float64); ok {
			evidence.WriteString(fmt.Sprintf("Activity Confidence: %.1f%%\n", confidence*100))
		}

		if workStart, ok := contextData["work_start_local"].(float64); ok {
			if workEnd, ok := contextData["work_end_local"].(float64); ok {
				evidence.WriteString(fmt.Sprintf("Work Hours: %.1f-%.1f local time\n", workStart, workEnd))
			}
		}

		if lunchStart, ok := contextData["lunch_start_local"].(float64); ok {
			if lunchEnd, ok := contextData["lunch_end_local"].(float64); ok {
				if lunchConf, ok := contextData["lunch_confidence"].(float64); ok {
					evidence.WriteString(fmt.Sprintf("Lunch Hours: %.1f-%.1f local time (%.1f%% confidence)\n",
						lunchStart, lunchEnd, lunchConf*100))
				}
			}
		}

		if sleepHours, ok := contextData["sleep_hours_utc"].([]int); ok && len(sleepHours) > 0 {
			evidence.WriteString(fmt.Sprintf("Sleep Hours UTC: %v\n", sleepHours))
		}

		if offset, ok := contextData["detected_gmt_offset"].(string); ok {
			evidence.WriteString(fmt.Sprintf("GMT Offset: %s\n", offset))
		}

		evidence.WriteString("\n")
	}

	// GITHUB USER PROFILE SECTION
	if userJSON, ok := contextData["github_user_json"]; ok {
		evidence.WriteString("## GITHUB USER PROFILE\n")
		if userBytes, err := json.MarshalIndent(userJSON, "", "  "); err == nil {
			evidence.WriteString(string(userBytes))
		}
		evidence.WriteString("\n\n")
	}

	// ORGANIZATIONS SECTION
	if orgs, ok := contextData["organizations"]; ok {
		evidence.WriteString("## ORGANIZATION MEMBERSHIPS\n")
		if orgBytes, err := json.MarshalIndent(orgs, "", "  "); err == nil {
			evidence.WriteString(string(orgBytes))
		}
		evidence.WriteString("\n\n")
	}

	// PULL REQUESTS SECTION
	if prs, ok := contextData["pull_requests"]; ok {
		evidence.WriteString("## RECENT PULL REQUEST TITLES\n")
		if prBytes, err := json.MarshalIndent(prs, "", "  "); err == nil {
			evidence.WriteString(string(prBytes))
		}
		evidence.WriteString("\n\n")
	}

	// LONGEST PR/ISSUE CONTENT SECTION (inline, not JSON)
	if title, ok := contextData["longest_pr_issue_title"].(string); ok && title != "" {
		evidence.WriteString("## LONGEST PR/ISSUE CONTENT\n")
		evidence.WriteString(fmt.Sprintf("Title: %s\n\n", title))

		if body, ok := contextData["longest_pr_issue_body"].(string); ok && body != "" {
			evidence.WriteString("Body:\n")
			evidence.WriteString(body)
			evidence.WriteString("\n\n")
		}
	}

	// WEBSITE CONTENT SECTION
	if websiteContent, ok := contextData["website_content"].(string); ok && websiteContent != "" {
		evidence.WriteString("## WEBSITE/BLOG CONTENT\n")
		evidence.WriteString(websiteContent)
		evidence.WriteString("\n\n")
	}

	// ISSUE COUNT
	if issueCount, ok := contextData["issue_count"].(int); ok {
		evidence.WriteString(fmt.Sprintf("## ADDITIONAL METRICS\n"))
		evidence.WriteString(fmt.Sprintf("Issue Count: %d\n", issueCount))
	}

	return evidence.String()
}
